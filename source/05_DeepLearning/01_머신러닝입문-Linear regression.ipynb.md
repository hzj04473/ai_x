# 머신러닝 입문: 선형 회귀 (Linear Regression) 완전 정복

안녕하세요! 머신러닝의 세계에 첫발을 내디딘 것을 환영합니다. 이 문서는 `01_머신러닝입문-Linear regression.ipynb` 노트북 파일의 코드를 한 줄 한 줄 따라가며, 머신러닝의 가장 기본적인 모델 중 하나인 **선형 회귀(Linear Regression)**를 이해하는 데 도움을 드리기 위해 작성되었습니다.

## 1. 머신러닝, 기존 프로그래밍과 무엇이 다른가요?

전통적인 프로그래밍에서는 개발자가 **명시적인 규칙(Rule)**을 코드로 작성하여 결과를 얻습니다. 예를 들어 "섭씨를 화씨로 바꾸려면 섭씨 온도에 1.8을 곱하고 32를 더하라"는 규칙을 직접 만드는 것이죠.

```python
# 전통적인 프로그래밍 방식
def celsius_to_fahrenheit(c):
    return c * 1.8 + 32

print(celsius_to_fahrenheit(10)) # 50.0
```

하지만 머신러닝은 다릅니다. 우리는 컴퓨터에 **데이터(Data)**와 그 데이터에 해당하는 **정답(Answer)**을 함께 보여줍니다. 그러면 컴퓨터가 데이터와 정답 사이의 **규칙(Rule)을 스스로 찾아냅니다.**

이 노트북에서는 섭씨 온도(데이터)와 그에 맞는 화씨 온도(정답)를 컴퓨터에 알려주고, "섭씨와 화씨 사이에는 어떤 관계가 있을까?"라는 질문에 대한 답, 즉 변환 규칙을 머신러닝 모델이 스스로 학습하도록 할 것입니다.

## 2. 핵심 개념 미리보기

코드를 살펴보기 전, 몇 가지 중요한 용어를 알아봅시다.

- **선형 회귀 (Linear Regression)**: 데이터 포인트들을 가장 잘 나타내는 '직선'을 찾는 알고리즘입니다. 이 직선을 통해 새로운 데이터가 주어졌을 때 결과를 예측할 수 있습니다. 수학식으로는 `Y = WX + b`로 표현됩니다.
- **모델 (Model)**: 데이터의 패턴을 학습하는 인공지능의 뇌라고 생각할 수 있습니다. 이 노트북에서는 '하나의 직선'이 바로 모델입니다.
- **독립 변수 (X)**: 입력값, 원인이 되는 데이터입니다. (예: 섭씨 온도)
- **종속 변수 (Y)**: 예측하려는 결과값, 정답 데이터입니다. (예: 화씨 온도)
- **가중치 (Weight, W)와 편향 (Bias, b)**: 모델이 학습을 통해 찾아내야 하는 값들입니다. 선형 회귀에서는 직선의 '기울기(W)'와 'y절편(b)'에 해당합니다.
- **손실 함수 (Loss Function)**: 모델의 예측이 얼마나 틀렸는지를 측정하는 함수입니다. 손실(loss)이 클수록 모델의 예측이 많이 틀렸다는 의미이며, 학습은 이 손실을 최소화하는 방향으로 진행됩니다. (예: 평균 제곱 오차, MSE)
- **옵티마이저 (Optimizer)**: 손실 함수를 최소화하기 위해 가중치(W)와 편향(b)을 어떻게 업데이트할지 결정하는 알고리즘입니다. (예: 경사 하강법, RMSprop)
- **에포크 (Epoch)**: 전체 훈련 데이터를 몇 번 반복해서 학습할지를 나타냅니다. 1 에포크는 전체 데이터를 한 번 모두 사용해 학습했음을 의미합니다.

## 3. 코드 해설: 단계별로 따라가기

이제 노트북의 코드를 순서대로 살펴보겠습니다.

### 3.1. 데이터 준비 및 전처리

머신러닝의 첫걸음은 언제나 좋은 데이터를 준비하는 것입니다.

**1. 데이터 생성**

```python
import numpy as np

# 0부터 99까지의 섭씨 온도 데이터를 만듭니다. (독립 변수 X)
data_C = np.arange(100) 

# 섭씨를 화씨로 변환하는 함수를 이용해 정답 데이터를 만듭니다. (종속 변수 Y)
data_F = celsius_to_fathercnhelt(data_C) 
```
- `numpy` 라이브러리를 사용해 0도부터 99도까지의 섭씨 온도 데이터를 생성합니다.
- 미리 만들어 둔 변환 함수를 사용해 각 섭씨 온도에 해당하는 화씨 온도(정답)를 계산합니다.

**2. 데이터 전처리 (스케일 조정)**

```python
# 2. 데이터 전처리 : 컴퓨터에게 학습시키기 위해 스케일 조정
scaled_data_C = data_C / 100.0
scaled_data_F = data_F / 100.0
```
- **왜 스케일 조정이 필요한가요?** 모델이 학습을 더 빠르고 안정적으로 진행하기 위함입니다. 데이터의 값들이 너무 크거나 들쭉날쭉하면, 학습 과정에서 손실 값이 한쪽으로 튀거나 불안정해질 수 있습니다. 모든 값을 0과 1 사이의 작은 숫자로 만들어주면 이런 문제를 방지하는 데 도움이 됩니다.
- 여기서는 모든 데이터를 100으로 나누어 0~1 사이의 값으로 변환했습니다.

### 3.2. 머신러NING 모델 만들기 (TensorFlow & Keras)

이제 본격적으로 인공지능 모델을 설계해 보겠습니다.

**1. 모델 구조 정의**

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 3. 모델 구성
model = Sequential()
model.add(Dense(units=1, input_shape=(1,)))
```
- `tensorflow`는 구글이 만든 강력한 머신러닝 라이브러리입니다.
- `Sequential`은 모델의 층(Layer)을 순서대로 차곡차곡 쌓을 수 있게 해주는 그릇과 같습니다.
- `Dense`는 가장 기본적인 신경망 층입니다. `Y = WX + b` 계산을 수행하는 핵심 부분입니다.
    - `units=1`: 출력값의 개수를 의미합니다. 화씨 온도 하나를 예측하므로 1로 설정합니다.
    - `input_shape=(1,)`: 입력값의 개수를 의미합니다. 섭씨 온도 하나를 입력받으므로 1로 설정합니다.

**2. 모델 학습 과정 설정 (컴파일)**

```python
# 4. 모델 학습과정 설정
model.compile(loss='mse', optimizer="rmsprop")
```
- `compile`은 모델이 어떻게 학습할지를 설정하는 단계입니다.
- `loss='mse'`: 손실 함수로 '평균 제곱 오차(Mean Squared Error)'를 사용합니다. 예측값과 실제값의 차이를 제곱하여 평균 낸 값으로, 오차를 측정하는 가장 일반적인 방법 중 하나입니다.
- `optimizer="rmsprop"`: 옵티마이저로 'RMSprop'을 사용합니다. 학습 과정에서 W와 b를 어떻게 조절하여 손실을 줄여나갈지 결정하는 알고리즘입니다.

### 3.3. 모델 학습 및 평가

**1. 모델 학습**

```python
# 5. 모델 학습 시키기
history = model.fit(scaled_data_C, scaled_data_F, epochs=1000, verbose=0)
```
- `fit` 함수는 실제 학습을 진행하는 명령입니다.
- `scaled_data_C` (입력)와 `scaled_data_F` (정답)를 모델에 보여주며 학습시킵니다.
- `epochs=1000`: 전체 데이터를 총 1000번 반복해서 학습합니다. 숫자가 클수록 더 정교하게 학습하지만, 시간이 오래 걸리고 너무 과하게 학습(과적합)할 위험도 있습니다.
- `verbose=0`은 학습 과정을 화면에 출력하지 않도록 설정한 것입니다.

**2. 학습 결과 확인 (손실 그래프)**

```python
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.show()
```
- 학습이 진행될수록(`epoch`이 늘어날수록) 손실(`loss`)이 어떻게 줄어드는지 그래프로 확인할 수 있습니다. 그래프가 오른쪽으로 갈수록 아래로 향한다면 학습이 잘 되고 있다는 의미입니다.

**3. 모델 평가 및 예측**

```python
# 6. 모델 평가
# 섭씨 10도를 스케일링하여 예측
prediction = model.predict([0.10]) 
print(prediction * 100) # 다시 원래 스케일로 복원

# 결과: [[50.]] 에 가까운 값이 나옵니다.
```
- `predict` 함수를 사용해 학습된 모델로 새로운 값을 예측할 수 있습니다.
- **중요!** 학습할 때 데이터를 스케일링했으므로, 예측할 때도 동일하게 스케일링된 값(10도 -> 0.1)을 넣어주어야 합니다.
- 예측된 결과 역시 스케일링된 값이므로, 다시 100을 곱해서 원래의 화씨 온도 값으로 복원합니다.

**4. 모델이 학습한 규칙 확인**

```python
# 모델이 학습한 W(가중치)와 b(편향) 확인
print(model.get_weights())
```
- `get_weights()` 함수로 모델이 학습한 W와 b 값을 직접 볼 수 있습니다.
- `F = C * 1.8 + 32` 라는 원래 공식과 비교해 봅시다.
- 데이터 스케일링(`C/100`, `F/100`)을 했으므로, 모델이 학습한 식은 `F/100 = (C/100) * W + b` 입니다.
- 이를 정리하면 `F = C * W + b * 100` 이 됩니다.
- 따라서 모델이 찾은 `W`는 1.8에 가까운 값이 되고, `b * 100`은 32에 가까운 값이 됩니다.

### 3.4. 노이즈가 있는 데이터로 실습

현실 세계의 데이터는 언제나 약간의 오차(노이즈)를 포함하고 있습니다. 노트북의 두 번째 파트에서는 일부러 데이터에 노이즈를 섞어서 학습을 진행합니다.

```python
# 노이즈가 섞인 데이터 생성
noise = np.random.uniform(-5.0, 5.0, size=data_F.shape)
data_F_noise = data_F + noise
```
- `np.random.uniform`을 사용해 -5에서 5 사이의 임의의 실수를 생성하여 원래 화씨 값에 더해줍니다.
- 이렇게 노이즈가 섞인 데이터를 사용해도, 선형 회귀 모델은 전체 데이터를 가장 잘 설명하는 '최적의 직선'을 찾아내는 것을 볼 수 있습니다. 이것이 바로 머신러닝의 강력함입니다.

## 4. 더 깊은 학습을 위한 자료

- **선형 회귀, 경사 하강법 시각 자료 (강력 추천)**
  - StatQuest - Linear Regression: [https://www.youtube.com/watch?v=nk2CQITm_eo](https://www.youtube.com/watch?v=nk2CQITm_eo)
- **TensorFlow 공식 튜토리얼**
  - 기본 회귀: [https://www.tensorflow.org/tutorials/keras/regression?hl=ko](https://www.tensorflow.org/tutorials/keras/regression?hl=ko)
- **손실 함수(MSE, MAE, RMSE) 설명**
  - 언제 MSE, MAE, RMSE를 사용하는가?: [https://jysden.medium.com/%EC%96%B8%EC%A0%9C-mse-mae-rmse%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94%EA%B0%80-c473bd831c62](https://jysden.medium.com/%EC%96%B8%EC%A0%9C-mse-mae-rmse%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94%EA%B0%80-c473bd831c62)

이 문서를 통해 머신러닝의 첫걸음을 성공적으로 떼셨기를 바랍니다. 꾸준히 여러 예제를 접하고 코드를 직접 수정해보면서 머신러닝과 친해져 보세요!
