# 📔 02_군집분석.ipynb 코드 해설

안녕하세요! 이 문서는 `02_군집분석.ipynb` 노트북 파일의 코드를 처음 머신러닝을 접하는 분들도 쉽게 이해할 수 있도록 설명하기 위해 만들어졌습니다. 각 코드 셀의 역할과 그 의미를 차근차근 알아보겠습니다.

---

## 1. 군집 분석 (Clustering) 이란?

군집 분석은 대표적인 **비지도 학습(Unsupervised Learning)** 방법 중 하나입니다. 비지도 학습이란, 정답이 없는 데이터로부터 스스로 패턴이나 구조를 찾아내는 방식입니다.

군집 분석은 데이터들을 비슷한 특징을 가진 그룹(군집, Cluster)으로 묶어주는 역할을 합니다. 예를 들어, 고객 데이터를 이용해 비슷한 구매 성향을 가진 고객들을 그룹으로 묶어 마케팅 전략을 세우거나, 뉴스 기사들을 비슷한 주제끼리 묶는 데 활용할 수 있습니다.

노트북에서는 대표적인 군집 분석 알고리즘 세 가지를 소개합니다.

-   **K-평균 (K-Means)**: 데이터를 K개의 그룹으로 나누는 가장 대중적인 알고리즘
-   **계층적 군집 (Hierarchical Clustering)**: 데이터 간의 거리를 기반으로 나무 형태(계층)로 군집을 만들어가는 알고리즘
-   **DBSCAN**: 데이터의 밀도를 기반으로 복잡한 모양의 군집도 찾아낼 수 있는 알고리즘

> 🔗 **참고 자료:**
>
> -   [Scikit-Learn 클러스터링 공식 문서 (영문)](https://scikit-learn.org/stable/modules/clustering.html)
> -   [군집 분석 소개 블로그 (한글)](https://hleecaster.com/ml-clustering-concept/)

---

## 2. K-평균 (K-Means) 군집 분석

K-평균은 가장 직관적이고 널리 사용되는 군집 알고리즘입니다.

**동작 원리:**

1.  `K`개의 중심점(Centroid)을 임의로 선택합니다.
2.  모든 데이터 포인트를 가장 가까운 중심점에 할당합니다.
3.  각 그룹의 중심점을 해당 그룹에 속한 데이터 포인트들의 평균값으로 이동시킵니다.
4.  중심점이 더 이상 변하지 않을 때까지 2번과 3번 과정을 반복합니다.

### 코드 해설

-   **가상 데이터 생성 (`make_classification`)**
    -   `sklearn.datasets`의 `make_classification` 함수를 사용해 군집 분석을 테스트할 가상의 2차원 데이터를 생성합니다.
    -   `n_samples`: 데이터 개수 (20개)
    -   `n_features`: 데이터의 특성(독립변수) 개수 (2개)
    -   `n_classes`: 정답 클래스 개수 (2개)
    -   `random_state`: 코드를 다시 실행해도 항상 같은 데이터를 생성하기 위한 값입니다.
    -   생성된 데이터를 `plt.scatter`를 이용해 시각화하여 데이터가 어떻게 분포되어 있는지 확인합니다.

-   **K-Means 모델 학습 (`KMeans`)**
    -   `sklearn.cluster`의 `KMeans`를 가져와 모델을 만듭니다.
    -   `n_clusters=2`: 데이터를 2개의 군집으로 나누겠다고 설정합니다.
    -   `init="k-means++"`: 초기 중심점을 서로 멀리 떨어지도록 설정하여 안정적인 결과를 얻기 위한 옵션입니다.
    -   `n_init=10`: 다른 초기 중심점으로 10번 반복 실행한 후 가장 좋은 결과를 선택합니다.
    -   `model.fit(X)`: 생성한 데이터 `X`를 모델에 넣어 학습시킵니다.

-   **결과 확인**
    -   `model.cluster_centers_`: 학습 후 결정된 2개 군집의 중심점 좌표를 보여줍니다.
    -   `model.labels_`: 각 데이터 포인트가 어떤 군집(0 또는 1)에 속하는지 보여줍니다.
    -   `plt.scatter`를 다시 사용해 군집화된 결과를 시각화합니다. 이때 각 점의 색깔(`c=pred`)을 모델이 예측한 군집 번호로 지정하고, 군집의 중심점은 네모(`marker="s"`)로 표시하여 한눈에 결과를 파악할 수 있게 합니다.

-   **데이터 정규화 (Normalization)**
    -   K-평균은 거리를 기반으로 하므로 데이터의 단위(scale)에 영향을 많이 받습니다.
    -   `Normalizer`를 사용해 각 데이터의 크기를 1로 정규화한 후 다시 K-평균을 실행하여 결과가 어떻게 달라지는지 비교합니다. 이는 데이터 전처리(preprocessing)의 중요성을 보여줍니다.

-   **K-Means의 한계**
    -   서로 다른 크기나 밀도를 가진 군집, 또는 원형이 아닌 복잡한 모양의 군집은 잘 찾아내지 못하는 단점이 있습니다.
    -   노트북에서는 크기가 다른 3개의 데이터 그룹을 만들어 K-평균을 적용했을 때, 큰 그룹을 억지로 나누는 등 예상과 다른 결과가 나오는 것을 보여줍니다.

> 🔗 **참고 자료:**
>
> -   [K-평균 알고리즘 시각적 설명 (Visualizing K-Means Clustering)](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)

---

## 3. 계층적 군집 (Hierarchical Clustering)

계층적 군집은 개별 데이터 포인트에서 시작해 가장 가까운 것끼리 순서대로 묶어나가는 '상향식(agglomerative)' 방식을 사용합니다. 모든 데이터가 하나의 큰 군집이 될 때까지 과정을 반복하며, 이 과정을 '덴드로그램(Dendrogram)'이라는 나무 형태의 다이어그램으로 시각화할 수 있습니다.

### 코드 해설

-   **데이터 준비 (`iris`)**
    -   `seaborn` 라이브러리에서 붓꽃(`iris`) 데이터를 불러옵니다.
    -   `LabelEncoder`를 사용해 붓꽃의 품종(`species`) 문자열(setosa, versicolor, virginica)을 숫자(0, 1, 2)로 변환합니다.

-   **계층적 군집 수행 (`linkage`)**
    -   `scipy.cluster.hierarchy`의 `linkage` 함수를 사용합니다.
    -   `method="complete"`: 군집과 군집 사이의 거리를 계산할 때, 각 군집에 속한 데이터 중 가장 먼 데이터 사이의 거리를 기준으로 합니다. (이 외에도 `single`, `average` 등 다양한 방법이 있습니다.)

-   **덴드로그램 시각화 (`dendrogram`)**
    -   `linkage`의 결과를 `dendrogram` 함수에 넣어 시각화합니다.
    -   덴드로그램의 y축은 군집 간의 거리를, x축은 데이터 포인트를 나타냅니다.
    -   가로선을 어디에 긋느냐에 따라 군집의 개수를 정할 수 있습니다. 예를 들어, 가로선이 3개의 세로선과 만나는 지점에 선을 그으면 3개의 군집이 생성됩니다.

-   **군집 형성 (`fcluster`)**
    -   덴드로그램을 특정 기준으로 잘라 실제 군집을 할당합니다.
    -   `criterion="distance", t=4`: 거리(y축)가 4인 지점에서 잘라 군집을 만듭니다.
    -   `criterion="maxclust", t=3`: 전체 군집 개수가 3개가 되도록 잘라 군집을 만듭니다.

> 🔗 **참고 자료:**
>
> -   [Scipy 계층적 군집 공식 문서 (영문)](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)

---

## 4. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

DBSCAN은 데이터가 밀집된 영역을 찾아 군집을 형성하는 '밀도 기반' 알고리즘입니다. K-평균과 달리 **복잡한 모양의 군집도 잘 찾아내고, 어떤 군집에도 속하지 않는 데이터(노이즈, 이상치)를 구분**해낼 수 있는 강력한 장점이 있습니다.

**핵심 개념:**

-   `eps` (epsilon): 하나의 데이터 포인트로부터의 거리를 나타내는 반경
-   `min_samples`: 하나의 군집으로 인정받기 위해 `eps` 반경 내에 있어야 할 최소 데이터 포인트 수

### 코드 해설

-   **가상 데이터 생성**
    -   `sin`, `cos` 함수와 노이즈를 이용해 구불구불한 모양의 데이터를 생성하여 DBSCAN의 성능을 테스트합니다.

-   **DBSCAN 모델 학습**
    -   `sklearn.cluster`의 `DBSCAN`을 가져와 모델을 만듭니다.
    -   `eps=0.3`: 반경을 0.3으로 설정합니다.
    -   `min_samples=10`: 반경 0.3 안에 최소 10개의 데이터가 있어야 군집으로 인정합니다.
    -   `db.fit()`으로 모델을 학습시킵니다.

-   **결과 확인**
    -   `db.labels_`: 각 데이터가 속한 군집 번호를 보여줍니다. **이때 -1은 노이즈(이상치)로 판별된 데이터**를 의미합니다.
    -   `set(labels)`를 이용해 생성된 군집의 개수와 노이즈의 개수를 계산합니다.
    -   `plt.scatter`로 결과를 시각화하면, DBSCAN이 K-평균으로는 찾기 어려운 구불구불한 모양의 군집을 성공적으로 찾아내는 것을 확인할 수 있습니다.

> 🔗 **참고 자료:**
>
> -   [DBSCAN 시각적 설명 (Visualizing DBSCAN Clustering)](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

---

## 5. 군집 모델 성능 평가

군집 분석은 정답이 없는 데이터를 다루므로 '정확도' 같은 명확한 평가 지표를 사용하기 어렵습니다. 하지만 만약 데이터의 실제 정답(클래스)을 알고 있다면, 군집화 결과가 실제 정답과 얼마나 유사한지 평가해볼 수 있습니다.

### 코드 해설

-   **데이터 및 모델 준비**
    -   붓꽃(`iris`) 데이터를 다시 사용합니다. `iris_X`는 특성 데이터, `iris_y`는 실제 품종(정답)입니다.
    -   K-평균 모델(`n_clusters=3`)을 학습시켜 예측값 `pred3`를 얻습니다.

-   **성능 평가 지표 (`sklearn.metrics`)**
    -   **`adjusted_rand_score` (ARI)**: 실제값과 예측값이 얼마나 유사한지를 측정합니다. 1에 가까울수록 완벽하게 일치함을 의미합니다.
    -   **`mutual_info_score` (MI)**: 두 군집화 결과 간의 상호 의존도를 측정합니다.
    -   코드에서는 실제 정답(`iris_y`)과 모델의 예측값(`pred3`)을 비교하여 점수를 계산합니다.
    -   **참고**: 모델이 예측한 군집 레이블(e.g., 0, 1, 2)과 실제 레이블(e.g., 0, 1, 2)은 숫자 자체는 의미가 다를 수 있습니다. (e.g., 예측 0번 군집이 실제 1번 품종일 수 있음) 노트북의 `np.choose` 부분은 이 레이블을 맞춰주기 위한 과정이지만, ARI나 MI 같은 지표들은 레이블 값이 달라도 자동으로 구조적 유사성을 평가해주므로 이 과정이 필수는 아닙니다.

> 🔗 **참고 자료:**
>
> -   [Scikit-Learn 군집 성능 평가 공식 문서 (영문)](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)

---

이 문서가 군집 분석을 이해하는 데 도움이 되셨기를 바랍니다!
