{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        \"\"\"<style>\n",
    "* {font-family:D2Coding;}\n",
    "div.container{width:87% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.CodeMirror {font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input { font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:3px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>ch2. LLM 활용의 기본 개념</span>\n",
    "\n",
    "# 1. LLM을 활용하여 답변 생성하기\n",
    "\n",
    "## 1) ollama 이용한 로컬 LLM 이용\n",
    "\n",
    "성능은 GPT, Claude 같은 모델보다 떨어지나, 개념 설명을 위해 open source 모델 사용\n",
    "\n",
    "### ollama.com 다운로드 → 설치 → 모델 pull\n",
    "\n",
    "- [deepseek다운로드] ollama pull deepseek-r1:1.5b (window키 + R → powershell)\n",
    "- [deepseek실행] ollama run deepseek-r1:1.5b\n",
    "- 다운위치 : /Users/teamkim/.ollama/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "result = llm.invoke(\"What is the capital of the Korea?\")\n",
    "# result = llm.invoke('너 한글로 물어보면, 한글로 답변이 가능해?')\n",
    "result  # 추론모델 <think>~<think>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ollama.com 다운로드 → 설치 → 모델 pull\n",
    "\n",
    "- [ollama다운로드] ollama pull llama3.2:1b\n",
    "- [ollama실행] ollama run llama3.2:1b\n",
    "- 다운위치 : /Users/teamkim/.ollama/\n",
    "- llama : 공식적으로 한글지원이 안됨 (llama3.1 405b 한글지원 가능 → llama3.3 70b)\n",
    "- exone : 공식적으로 한글지원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "result = llm.invoke(\"What is the capital of the Korea?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"한국 수도는 어디예요?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) openai 활용\n",
    "\n",
    "- pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# result = llm.invoke(\"What is the capital of the Korea?\")\n",
    "# result 에러이유 : OPENAI_API_KEY 환경변수 부재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# import os\n",
    "load_dotenv()\n",
    "# os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩에서 OPENAI_API_KEY 읽어오기 (.env 못씀)\n",
    "# 보안키 추가 후\n",
    "# from google.colab import userdata\n",
    "# userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    # openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "llm.invoke(\"What is the capital of the Korea? Answer me in Korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델의 키가 OPENAI_API_KEY 는 아님\n",
    "# Claude → Anthropic\n",
    "# Azure → Azure OpenAI\n",
    "# upstage, Bedrock : 에러 메시지 참조하여 환경변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import AzureOpenAI\n",
    "# llm = AzureOpenAI(model=\"gpt-4.1-nano\")\n",
    "# llm.invoke(\"What is the capital of the Korea? Answer me in Korean\")\n",
    "# 에러를 내면, OPENAI_API_VERSION 환경변수가 필요하는 메세지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "# llm.invoke(\"What is the capital of the Korea? Answer me in Korean\")\n",
    "# 에러 메시지를 봐도 환경변수 이름을 알 수 없음 → ChatAnthropic 검색 후, LangChain docs 에서 명시한 ANTHROPIC_API_KEY 이름의 환경변수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 렝체인 스타일로 프롬프트 작성하기\n",
    "\n",
    "- 프롬프트 : llm 호출시 쓰는 질문\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "# llm.invoke(0) # error : PromptValue, str, or list of BaseMessages.\n",
    "\n",
    "# ! 프롬프트 타입 : 스트링, PromptValue, BaseMessage리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 기본 프롬프트 템플릿 사용\n",
    "\n",
    "- PromptTemplate을 사용하여 변수가 포함된 템플릿 작성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    # template=\"What is the {thing} of the {country}?\", # {} 안의 값을 새로운 값으로 할당 가능\n",
    "    # input_variables = [\"country\", \"thing\"]\n",
    "    template=\"What is the capital of the {country}?\",  # {} 안의 값을 새로운 값으로 할당 가능\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\": \"Korea\"})\n",
    "print(prompt)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 메시지 기반 프롬프트 작성\n",
    "\n",
    "- BaseMessage리스트\n",
    "- BaseMessage 상속 받는 클래스 : AIMessage, HumanMessage, SystemMessage, ToolMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "message_list = [\n",
    "    # 예상 질물과 답변\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"),\n",
    "    \n",
    "    HumanMessage(content=\"What is the capital of the Italy?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Rome.\"),\n",
    "    HumanMessage(content=\"What is the capital of the Korea?\"),\n",
    "    AIMessage(content=\"The capital of Korea is Seoul.\"),\n",
    "    HumanMessage(content=\"What is the capital of the France?\"),\n",
    "]\n",
    "\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseMessage list로 하면, 랭페인화도 않되고, ChatPromptTemplate 사용 불가\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "message_list = [\n",
    "    # 예상 질물과 답변\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"),\n",
    "    \n",
    "    HumanMessage(content=\"What is the capital of the Italy?\"),\n",
    "    AIMessage(content=\"The capital of Italy is Rome.\"),\n",
    "    HumanMessage(content=\"What is the capital of the Korea?\"),\n",
    "    AIMessage(content=\"The capital of Korea is Seoul.\"),\n",
    "    HumanMessage(content=\"What is the capital of the {country}?\"),\n",
    "]\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages(message_list)\n",
    "\n",
    "prompt = chatPromptTemplate.invoke({\"country\": \"Korea\"})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) ChatPromptTemplate 사용\n",
    "\n",
    "- BaseMessage 리스트 → 튜플 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 BaseMessage 를 수정\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant!.\"),\n",
    "        (\"human\", \"What is the capital of the {country}?\"),\n",
    "    ]\n",
    ")\n",
    "country = input(\"어느나라 수도가 궁급하세요\")\n",
    "prompt = chatPromptTemplate.invoke({\"country\": country})\n",
    "print(\"프롬프트 :\", prompt)\n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어로... - 정확성 떨어짐\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 수도를 맞추는 전문가야. 꼭 수도만 답변해라\"),\n",
    "        (\"human\", \"{country}의 수도가 어디예요?\"),\n",
    "    ]\n",
    ")\n",
    "country = input(\"어느나라 수도가 궁급하세요\")\n",
    "prompt = chatPromptTemplate.invoke({\"country\": country})\n",
    "print(\"프롬프트 :\", prompt)\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 답변 형식을 컨트롤하기\n",
    "\n",
    "- invoke 실행결과 AIMessage() → String이나 JSON, 객체 : outputParser 이용 렝체인\n",
    "\n",
    "## 1) 문자열 출력 파서 이용\n",
    "\n",
    "- StrOutputParser를 사용하여, LLM출력(AIMessage)을 단순 문자열로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of the {country}. Return the name of the city only.\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({\"country\": \"Korea\"})\n",
    "print(\"프롬프트 :\", prompt)\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "print(\"llm 결과 :\", result)\n",
    "\n",
    "# 문자열 풀력 파서를 이용하여 llm 응답을 단순 문자열 변환\n",
    "output_parser = StrOutputParser()\n",
    "print(\"문자열 파서 결과 :\", output_parser.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\": \"Korea\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate(변수설정)  → ChatPromptTemplate(변수설정, system 과 모범답안 지정)\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "chatPrompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant with expertise in Seoul Korea\"),\n",
    "        (\"human\", \"What is the capital of the {country}? Return the name of the city only.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(llm.invoke(chatPrompt_template.invoke({\"country\": \"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Json 출력 파서 이용\n",
    "\n",
    "\n",
    "- json()으로 응답하기를 원하지만, 우선 어떤 형식으로 반환 되는지 확인\n",
    "- {'name':'홍', 'age':22 ... } (json)) {'name':'홍', age:22} (dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "country_detail_prompt = PromptTemplate(\n",
    "    template=\"\"\"Give follwing information about {country}\n",
    "        - Capital\n",
    "        - Population\n",
    "        - Language\n",
    "        - Currency\n",
    "        return it is JSON format and return the JSON dictionary only.\n",
    "    \"\"\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt = country_detail_prompt.invoke({\"country\": \"Korea\"})\n",
    "print(type(prompt), prompt)\n",
    "\n",
    "# JSON output 파서\n",
    "output_parser = JsonOutputParser()\n",
    "ai_message = llm.invoke(prompt)\n",
    "\n",
    "print(type(prompt), ai_message)\n",
    "\n",
    "json_result = output_parser.invoke(ai_message)\n",
    "print(type(json_result), json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template=\"\"\"Give follwing information about {country}\n",
    "        - Capital\n",
    "        - Population\n",
    "        - Language\n",
    "        - Currency\n",
    "        return it is JSON format and return the JSON dictionary only.\n",
    "    \"\"\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "info = output_parser.invoke(llm.invoke(country_detail_prompt.invoke({\"country\": \"Korea\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 구조화된 출력 사용\n",
    "\n",
    "- Pydantic 모델을 사용하여, LLM출력을 구조화된 형식으로 받기 (JsonParser 보다 훨씬 안정적)\n",
    "- Pydantic : 데이터 유효성 검사, 설정관리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class User:\n",
    "    def __init__(self, id, name, is_active=True):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.is_active = is_active\n",
    "\n",
    "    # def __str__(self):\n",
    "    #     return f\"User(id={self.id}, name={self.name}, is_active={self.is_active})\"\n",
    "\n",
    "user = User(1, \"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class User(BaseModel):\n",
    "    # gt = 0:id>0, ge: 0:id >=0, lt : 0:id<0, le : 0:le<=0\n",
    "    id: int = Field(gt=0, description=\"User ID\")\n",
    "    name: str = Field(min_length=2, description=\"User Name\")\n",
    "    is_active: bool = Field(default=True, description=\"User Active\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"User(id={self.id}, name={self.name}, is_active={self.is_active})\"\n",
    "    \n",
    "user = User(id=\"1\", name=\"홍길동\")\n",
    "user    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template=\"\"\"Give follwing information about {country}\n",
    "        - Capital\n",
    "        - Population\n",
    "        - Language\n",
    "        - Currency\n",
    "        return it is JSON format and return the JSON dictionary only.\n",
    "    \"\"\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "class CountryDetail(BaseModel):\n",
    "    capital: str = Field(description=\"The capital of the country\")\n",
    "    population: int = Field(description=\"The population of the country\")\n",
    "    language: str = Field(description=\"The language of the country\")\n",
    "    currency: str = Field(description=\"The currency of the country\")\n",
    "\n",
    "# 출력 형식 곤리\n",
    "structedllm = llm.with_structured_output(CountryDetail)\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "# output_parser.invoke(llm.invoke(country_detail_prompt.invoke({\"country\": \"Korea\"})))\n",
    "# output_parser\n",
    "\n",
    "info = structedllm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"}))\n",
    "type(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)\n",
    "print(info.capital)\n",
    "print(info.population)\n",
    "print(info.language)\n",
    "print(info.currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('info.json :', info.model_dump_json()) # json()\n",
    "print('info.dict :', info.model_dump()) # dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LCEL을 활용한 렝체인 생성하기\n",
    "\n",
    "## 1) 문자열 출력 파서 사용\n",
    "- invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\",temparature=0) # 0: 일관된 답볍\n",
    "\n",
    "# 명시적인 지시사항이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of the {country}. Return the name of the city only.\",\n",
    "    input_variables = [\"country\"],\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) LCEL을 사용한 간단한 체인 구성\n",
    "\n",
    "- 파이프연산자(|) 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 → llm → 출력파서를 연결\n",
    "capital_chain = prompt_template | llm | output_parser\n",
    "\n",
    "# 생성된 체인 invoke\n",
    "capital_chain.invoke({\"country\":\"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(capital_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 복합 체힌 구성\n",
    "\n",
    "- 여러단계의 추론이 필요한 경우 (체인 연결)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나라 설명 → 나라명\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "country_propmt = PromptTemplate(\n",
    "    template = \"\"\"Guess the name of the country based on the following information: : {information} \n",
    "    Return the name of the country only\"\"\",\n",
    "    input_variables=[\"information\"]\n",
    ")\n",
    "\n",
    "output_parser.invoke(llm.invoke(country_propmt.invoke({\"information\": \"The country is very famous for it's wine\"})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나라명 추측 체인 생성\n",
    "country_chain = country_propmt | llm | output_parser\n",
    "print(type(country_chain))\n",
    "\n",
    "country_chain.invoke({\"information\": \"The country is very famous for it's wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나라설명 → (나라명 →) 그 나라 수도\n",
    "final_chain = country_chain | capital_chain\n",
    "# type(final_chain)\n",
    "final_chain.invoke({\"information\": \"The country is very famous for it's wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나라설명 → 나라명 → 수도 체인 생성 \n",
    "final_chain = {\"country\":country_chain} | capital_chain\n",
    "\n",
    "final_chain.invoke({\"The country is very famous for it's wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "final_chain = {\"information\": RunnablePassthrough()} | {\"country\":country_chain} | capital_chain\n",
    "\n",
    "final_chain.invoke({\"information\": \"The country is very famous for it's wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿에 변수가 2개인 경우\n",
    "# 나라 설명 → 나라명\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "country_propmt = PromptTemplate(\n",
    "    template = \"\"\"Guess the name of the country in the {continent} based on the following information: : {information} \n",
    "    Return the name of the country only\"\"\",\n",
    "    input_variables=[\"information\", \"continent\"]\n",
    ")\n",
    "\n",
    "# output_parser.invoke(llm.invoke(country_propmt.invoke({\"information\": \"The country is very famous for it's wine\",\"continent\":\"Europe\"})))\n",
    "country_chain = country_propmt | llm | output_parser\n",
    "\n",
    "country_chain.invoke({\"information\": \"The country is very famous for it's wine\",\"continent\":\"Europe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = {\"country\":country_chain} | capital_chain\n",
    "\n",
    "final_chain.invoke({\"information\": \"The country is very famous for it's wine\",\"continent\":\"Europe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "final_chain = {\"information\": RunnablePassthrough()} | {\"country\":country_chain} | capital_chain\n",
    "\n",
    "final_chain.invoke({\"information\": \"The country is very famous for it's wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "final_chain = {\"information\": RunnablePassthrough()} | {\"country\":country_chain} | capital_chain\n",
    "\n",
    "final_chain.invoke({\"information\": \"The country is very famous for it's wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 나라명 → (제일 유명한 음식 →) 음식의 레시피\n",
    "# 다음의 생성형 ai를 렝체인으로 구현하시오.\n",
    "# ▪ 원하는 모델을 이용하여 나라를 입력하면 해당 나라의 가장 유명한 음식을 추천하는 렝체인을 구현\n",
    "# (food_chain))\n",
    "# ▪ 음식을 입력하면 레시피를 생성하는 렝체인을 구현(recipe_chain)\n",
    "# ▪ 위의 두 체인을 연결하여, 나라이름을 입력받아, 해당 나라의 가장 유명한 음식의 레시피를 생성하는 렝체인 프로그램을 구현하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibimbap.\n",
      "Ingredients:\n",
      "\n",
      "* 1 cup of mixed vegetables (such as bean sprouts, zucchini, carrots, and spinach)\n",
      "* 2 cups of cooked white rice\n",
      "* 1/4 cup of toasted sesame seeds\n",
      "* 1/4 cup of chopped green onions\n",
      "* 2 tablespoons of gochujang sauce\n",
      "* 2 tablespoons of soy sauce\n",
      "* 1 tablespoon of rice vinegar\n",
      "* 1 teaspoon of ground black pepper\n",
      "* 1/2 teaspoon of crushed red pepper (optional)\n",
      "* 2 eggs, beaten\n",
      "* 1 cup of diced zucchini\n",
      "* 1 cup of diced cucumber\n",
      "* 1 cup of diced carrots\n",
      "* 1/4 cup of chopped scallions\n",
      "* 1 tablespoon of toasted sesame oil\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Cook the rice and let it cool.\n",
      "2. In a large bowl, combine the mixed vegetables, cooked white rice, toasted sesame seeds, green onions, gochujang sauce, soy sauce, rice vinegar, black pepper, crushed red pepper (if using), and beaten eggs.\n",
      "3. Add the diced zucchini, cucumber, and carrots to the bowl and toss gently.\n",
      "4. Ladle the Bibimbap onto a plate and top with additional sesame seeds, green onions, and scallions if desired.\n",
      "5. Serve immediately.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\",temparature=0) # 0: 일관된 답변\n",
    "\n",
    "\n",
    "country_input = input(\"Enter the country name: \")\n",
    "\n",
    "food_prompt = PromptTemplate(\n",
    "    template = \"\"\"What is the most famous food in {country_name}?\n",
    "    Return only the name of the food.\"\"\",\n",
    "    input_variables=[\"country_name\"]\n",
    ")\n",
    "\n",
    "\n",
    "food_chain = food_prompt | llm | StrOutputParser()\n",
    "\n",
    "food_result =food_chain.invoke({\"country_name\": country_input})\n",
    "print(food_result)\n",
    "\n",
    "\n",
    "recipe_prompt = PromptTemplate(\n",
    "    template = \"\"\"What is the recipe for {food_name}?\n",
    "    Return only the recipe.\"\"\",\n",
    "    input_variables=[\"food_name\"]\n",
    ")\n",
    "\n",
    "recipe_chain = recipe_prompt | llm | StrOutputParser()\n",
    "\n",
    "recipe_result = recipe_chain.invoke({\"food_name\": food_result})\n",
    "print(recipe_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
